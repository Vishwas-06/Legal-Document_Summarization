{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AdamW\n\n\n# Step 1: Load the Dataset\n# dataset_path = '/content/Datasetnew from hugging face.xlsx'\ndf = pd.read_excel(\"/kaggle/input/legal2/Datasetnew from hugging face.xlsx\")\n\n# Step 2: Split the Dataset into Train, Test, and Validation Sets\ntrain_text, test_text, train_summary, test_summary = train_test_split(\n    df['judgement'], df['summary'], test_size=0.4, random_state=42\n)\ntest_text, val_text, test_summary, val_summary = train_test_split(\n    test_text, test_summary, test_size=0.5, random_state=42\n)\n\n# Load the Pegasus model and tokenizer\nmodel_name = 'google/pegasus-xsum'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name)\n\n# Step 4: Tokenize the Data\ntrain_encodings = tokenizer(list(train_text), truncation=True, padding=True)\ntrain_labels = tokenizer(list(train_summary.astype(str)), truncation=True, padding=True)\n\nval_encodings = tokenizer(list(val_text), truncation=True, padding=True)\nval_labels = tokenizer(list(val_summary.astype(str)), truncation=True, padding=True)\n\ntest_encodings = tokenizer(list(test_text), truncation=True, padding=True)\ntest_labels = tokenizer(list(test_summary.astype(str)), truncation=True, padding=True)\n\n\n# Step 5: Create PyTorch Datasets\nclass SummaryDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels['input_ids'])\n\ntrain_dataset = SummaryDataset(train_encodings, train_labels)\nval_dataset = SummaryDataset(val_encodings, val_labels)\ntest_dataset = SummaryDataset(test_encodings, test_labels)\n\n# Step 6: Fine-tune the BART Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} - Training Loss: {avg_loss:.4f}\")\n\n    # Step 7: Evaluation on Validation Set\n    model.eval()\n\n    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n\n    total_val_loss = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T14:32:16.319833Z","iopub.execute_input":"2023-06-12T14:32:16.320162Z","iopub.status.idle":"2023-06-12T15:10:02.507432Z","shell.execute_reply.started":"2023-06-12T14:32:16.320136Z","shell.execute_reply":"2023-06-12T15:10:02.506277Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"396012066d3745fc97287a107872a95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2955d7c2d6e4c249c329f239ef82ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c432f842927491d927e8cd8260e9f95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176cbbe3af5045f3b4076b372bf22825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af12648c1ce7445b8f8c4d03a1ba8340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a57b942be744419ba16b11f555fd629"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training Loss: 3.7115\nEpoch 1 - Validation Loss: 3.1605\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 10: Save the Model\nmodel_save_path = \"/kaggle/working/Pegasus_model_hfd.h5\"\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to: {model_save_path}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T15:20:44.459649Z","iopub.execute_input":"2023-06-12T15:20:44.460440Z","iopub.status.idle":"2023-06-12T15:20:50.349982Z","shell.execute_reply.started":"2023-06-12T15:20:44.460408Z","shell.execute_reply":"2023-06-12T15:20:50.348984Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Model saved to: /kaggle/working/Pegasus_model_hfd.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntest_loader = DataLoader(test_dataset, batch_size=3, shuffle=False)\n\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=200)\n        batch_preds = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n        predictions.extend(batch_preds)\n\n\ntest_summary_list = test_summary.tolist()\n\nfor i in range(len(predictions)):\n    print(f\"Target: {test_summary_list[i]}\")\n    print(f\"Prediction: {predictions[i]}\")\n    print(\"=\" * 5)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T15:36:22.581420Z","iopub.execute_input":"2023-06-12T15:36:22.581765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2023-06-12T15:35:22.043215Z","iopub.execute_input":"2023-06-12T15:35:22.043582Z","iopub.status.idle":"2023-06-12T15:35:34.372640Z","shell.execute_reply.started":"2023-06-12T15:35:22.043555Z","shell.execute_reply":"2023-06-12T15:35:34.371469Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from rouge import Rouge\n\nreferences = []\nhypotheses = []\n\nfor i in range(len(predictions)):\n    reference = test_summary_list[i]\n    hypothesis = predictions[i]\n    references.append(reference)\n    hypotheses.append(hypothesis)\n\nrouge = Rouge()\n\nscores = rouge.get_scores(hypotheses, references, avg=True)\n\nrouge_1_score = scores['rouge-1']\nrouge_2_score = scores['rouge-2']\nrouge_l_score = scores['rouge-l']\n\nprint(f\"ROUGE-1 Score: {rouge_1_score}\")\nprint(f\"ROUGE-2 Score: {rouge_2_score}\")\nprint(f\"ROUGE-L Score: {rouge_l_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T15:36:00.410008Z","iopub.execute_input":"2023-06-12T15:36:00.410956Z","iopub.status.idle":"2023-06-12T15:36:00.486996Z","shell.execute_reply.started":"2023-06-12T15:36:00.410919Z","shell.execute_reply":"2023-06-12T15:36:00.485769Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m hypotheses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(predictions)):\n\u001b[0;32m----> 8\u001b[0m     reference \u001b[38;5;241m=\u001b[39m \u001b[43mtest_summary_list\u001b[49m[i]\n\u001b[1;32m      9\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m predictions[i]\n\u001b[1;32m     10\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend(reference)\n","\u001b[0;31mNameError\u001b[0m: name 'test_summary_list' is not defined"],"ename":"NameError","evalue":"name 'test_summary_list' is not defined","output_type":"error"}]}]}